{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30bb0166",
   "metadata": {},
   "source": [
    "# Opioid Misuse Prediction Models\n",
    "\n",
    "*Yiyu Wang 2024/04/19*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "figures_dir = '../figures/'\n",
    "model_dir = '../model_results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_PREDICTOR_COLUMNS=['k23_age', 'demo_hispanic', 'demo_ethnicity_1', 'demo_ethnicity_2', 'demo_ethnicity_3',\n",
    "       'demo_ethnicity_4', 'demo_ethnicity_5', 'demo_ethnicity_6',\n",
    "       'demo_ethnicity_99', 'demo_gender_1', 'demo_gender_2', 'demo_gender_99',\n",
    "       'demo_income', 'demo_education', 'demo_legal', 'demo_employment___1',\n",
    "       'demo_employment___2', 'demo_employment___3', 'demo_employment___4',\n",
    "       'demo_employment___5', 'demo_employment___6', 'demo_employment___7',\n",
    "       'demo_employment___8', 'demo_employment___9', 'demo_employment___99',\n",
    "       'demo_disability', 'demo_marital', 'mh_accident', 'mh_pain_duration',\n",
    "       'promis_pi_01', 'promis_pi_02', 'promis_pi_03', 'opioid_years_v2',\n",
    "       'meds_more_v2', 'PainInT', 'AngerT', 'AnxietyT', 'DepressT', 'FatigueT',\n",
    "       'GlobalpT', 'GlobalmT', 'PhyFxT', 'SleepDisT', 'audittot', 'AUDITpos',\n",
    "       'pcstotal', 'pcs_help', 'pcs_rum', 'pcs_mag', 'dasttot', 'c_eactotl',\n",
    "       'aeqtot', 'ctq_emo_abu', 'ctq_phy_abu', 'ctq_emo_neg', 'ctq_phy_neg',\n",
    "       'ctq_sex_abu', 'ctqtot', 'mh_psychological_yes_binary']\n",
    "\n",
    "rename_dict = {'k23_age': 'age', \n",
    "              'demo_hispanic': 'Hispanic', \n",
    "              'demo_ethnicity_1':'Asian',\n",
    "              'demo_ethnicity_2':'Caucasian', \n",
    "              'demo_ethnicity_3':'NativeHawaiian', \n",
    "              'demo_ethnicity_4':'Black', \n",
    "              'demo_ethnicity_5':'AmericanIndian', \n",
    "              'demo_ethnicity_6':'MoreThanOneRace', \n",
    "              'demo_ethnicity_99':'OtherEthnicity',\n",
    "              'demo_income':'income', 'demo_education':'education',\n",
    "              'demo_legal':'legal', \n",
    "              'demo_gender_1': 'male', 'demo_gender_2': 'female', 'demo_gender_99':'OtherGender',\n",
    "              'demo_employment___1':'part_time', 'demo_employment___2':'full_time',\n",
    "              'demo_employment___3':'not_employed', 'demo_employment___4':'homemaker',\n",
    "              'demo_employment___5':'temp_unemployed',\n",
    "              'demo_employment___6':'unemployed', 'demo_employment___7':'looking_unemployed',\n",
    "              'demo_employment___8':'disabled', 'demo_employment___9':'retired',\n",
    "              'demo_employment___99':'OtherEmployment', 'demo_disability':'disability',\n",
    "              'demo_marital':'marital', 'opioid_years_v2': 'opioid_years', 'meds_more_v2':'meds_more',\n",
    "              'mh_accident':'accident', 'mh_pain_duration':'pain_duration', 'mh_psychological_yes_binary':'psychological_treatment_yes',\n",
    "              'promis_pi_01':'past_pain_intensity', 'promis_pi_02':'worst_pain_intensity', 'promis_pi_03':'current_pain_intensity',\n",
    "              'PainInT':'PainInterference', 'AngerT':'Anger', 'AnxietyT':'Anxiety', 'DepressT':'Depression', 'FatigueT':'Fatigue',\n",
    "              'GlobalpT':'GlobalPhysical', 'GlobalmT':'GlobalMental', 'PhyFxT':'PhysicalFunction', 'SleepDisT':'SleepDisturbance',\n",
    "              'audittot':'AlcoholUseScore', 'AUDITpos':'AlcoholUserBinary', \n",
    "              'pcstotal':'PainCatastrophizing_total', 'pcs_help':'PCS_helplessness', 'pcs_rum':'PCS_rumination', 'pcs_mag':'PCS_magnification',\n",
    "              'dasttot':'DrugUseScore', 'c_eactotl':'CocaineUseScore', 'aeqtot':'AmbivalenceEmotion',\n",
    "              'ctqtot': 'ChildhoodTrauma_total', 'ctq_emo_abu':'CTQ_EmotionalAbuse', 'ctq_phy_abu':'CTQ_PhysicalAbuse', 'ctq_emo_neg':'CTQ_EmotionalNeglect', 'ctq_phy_neg':'CTQ_PhysicalNeglect','ctq_sex_abu':'ctq_SexualAbuse'}\n",
    "\n",
    "PREDICTOR_COLUMNS = [rename_dict[col] for col in raw_PREDICTOR_COLUMNS]\n",
    "print(PREDICTOR_COLUMNS)\n",
    "print('n predictors =', len(PREDICTOR_COLUMNS))\n",
    "test_size = 0.2\n",
    "SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ceaf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c229c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(df, split_data = 'split', outcome = 'ouddx', predictor_columns = PREDICTOR_COLUMNS, test_size = 0.2, SEED = SEED):\n",
    "    if split_data == 'cohort':\n",
    "\n",
    "        y_train, y_test, y_val = df.loc[df['cohort'] == 'train', outcome], df.loc[df['cohort'] == 'test', outcome], df.loc[df['cohort'] == 'val', outcome]\n",
    "        X_train, X_test, X_val = df.loc[df['cohort'] == 'train', predictor_columns], df.loc[df['cohort'] == 'test', predictor_columns], df.loc[df['cohort'] == 'validation', predictor_columns]\n",
    "\n",
    "        print(X_train.shape, X_test.shape, X_val.shape)\n",
    "    elif split_data == 'split':\n",
    "        X = df[predictor_columns]\n",
    "        y = df[outcome]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)\n",
    "        \n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = [], [], [], []\n",
    "        print('split_data must be either \"split\" or \"cohort\"')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    # return {'X_train': X_train,'X_test':X_test, 'y_train': y_train, 'y_test':y_test}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011bb8e3-282c-4d0c-8b2f-5b45e393e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir + 'M_K23_ML_reduced_imputed.csv')\n",
    "df = df.rename(columns=rename_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split select participants with commtot > 9\n",
    "which_group = 'high'    \n",
    "if which_group == 'high':\n",
    "    df = df.loc[df['commtot'] > 9].reset_index(drop=True)\n",
    "    df.to_csv(data_dir + f'M_K23_ML_group-{which_group}_reduced_imputed.csv', index=False)\n",
    "elif which_group == 'low':\n",
    "    df = df.loc[df['commtot'] <= 9].reset_index(drop=True)\n",
    "    df.to_csv(data_dir + f'M_K23_ML_group-{which_group}_reduced_imputed.csv', index=False)\n",
    "elif which_group == 'all':\n",
    "    pass\n",
    "    df.to_csv(data_dir + f'M_K23_ML_group-{which_group}_reduced_imputed.csv', index=False)\n",
    "\n",
    "df.describe().to_csv(data_dir + f'M_K23_ML_group-{which_group}_reduced_imputed_describe.csv')\n",
    "df.describe()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_cv(model, df, split, outcome, n_splits=100, test_size=0.3, seed_start=SEED, predictor_columns=PREDICTOR_COLUMNS):\n",
    "    auc_scores, acc_scores, mse_scores, r_scores = [], [], [], []\n",
    "    seeds = np.arange(seed_start, seed_start + n_splits)  # Define seeds for repeatability\n",
    "    models = []\n",
    "    for seed in seeds:\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = split_data(df, split_data = split, outcome = outcome, predictor_columns=predictor_columns, test_size = test_size, SEED = seed)\n",
    "        \n",
    "         # set up model\n",
    "        if model == None:\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=200,          # Number of trees in the forest\n",
    "                max_depth=4,              # Maximum depth of the trees\n",
    "                min_samples_split=4,       # Minimum number of samples required to split an internal node\n",
    "                min_samples_leaf=2,        # Minimum number of samples required to be at a leaf node\n",
    "                random_state=seed,            # Ensures a deterministic outcome for reproducibily\n",
    "            )\n",
    "        else:\n",
    "            model = model\n",
    "            model.random_state = seed \n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        models.append(model)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        # Evaluate the model\n",
    "        if outcome == 'commtot':\n",
    "            mse_score = mean_squared_error(y_test, y_pred)\n",
    "            mse_scores.append(mse_score)\n",
    "\n",
    "            r_score = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "            r_scores.append(r_score)\n",
    "\n",
    "        elif outcome == 'ouddx':\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            auc_scores.append(auc_score)\n",
    "\n",
    "            accuracy = np.mean(y_pred == y_test)\n",
    "            acc_scores.append(accuracy)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Invalid outcome: {outcome}')    \n",
    "\n",
    "    # collect the results   \n",
    "    if outcome == 'commtot':\n",
    "        # Calculate mean mse and 95% CI\n",
    "        mean_mse = np.mean(mse_scores)\n",
    "        mse_ci_lower, mse_ci_upper = np.percentile(mse_scores, [2.5, 97.5])\n",
    "        mean_r = np.mean(r_scores)\n",
    "        r_ci_lower, r_ci_upper = np.percentile(r_scores, [2.5, 97.5])\n",
    "        # Calculate mean r and 95% CI\n",
    "        r2_scores = [r**2 for r in r_scores]\n",
    "        mean_r2 = np.nanmean(r2_scores)\n",
    "        result = {\n",
    "            'models': models,\n",
    "            'mean_mse': mean_mse,\n",
    "            'mse_ci_lower': mse_ci_lower,\n",
    "            'mse_ci_upper': mse_ci_upper,\n",
    "            'mean_r': mean_r,\n",
    "            'median_r': np.nanmedian(r_scores), \n",
    "            'r_ci_lower': r_ci_lower,\n",
    "            'r_ci_upper': r_ci_upper,\n",
    "            'mse_scores': mse_scores,\n",
    "            'r_scores': r_scores,\n",
    "            'r2_scores': r2_scores,\n",
    "            'mean_r2': mean_r2,\n",
    "            'median_r2': np.nanmedian(r2_scores)\n",
    "        }\n",
    "      \n",
    "    elif outcome == 'ouddx':\n",
    "        # Calculate mean AUC and 95% CI\n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        auc_ci_lower, auc_ci_upper = np.percentile(auc_scores, [2.5, 97.5])\n",
    "\n",
    "        mean_accuracy = np.mean(acc_scores)\n",
    "        accuracy_ci_lower, accuracy_ci_upper = np.percentile(acc_scores, [2.5, 97.5])\n",
    "        \n",
    "        result = {\n",
    "            'models': models,\n",
    "            'mean_auc': mean_auc,\n",
    "            'auc_ci_lower': auc_ci_lower,\n",
    "            'auc_ci_upper': auc_ci_upper,\n",
    "            'mean_accuracy': mean_accuracy,\n",
    "            'accuracy_ci_lower': accuracy_ci_lower,\n",
    "            'accuracy_ci_upper': accuracy_ci_upper,\n",
    "            'auc_scores': auc_scores,\n",
    "            'acc_scores': acc_scores,\n",
    "        }   \n",
    "           \n",
    "    return result\n",
    "\n",
    "\n",
    "def mc_cv_ml_pipeline(model, df, split, outcome, test_size=0.3, n_split=100, seed_start=0, predictor_columns = PREDICTOR_COLUMNS):\n",
    "    # Split the dataframe into X (features) and y (target)\n",
    "    \n",
    "    result = monte_carlo_cv(model, df, split, outcome, n_splits=n_split, test_size=test_size, seed_start=seed_start, predictor_columns=predictor_columns)\n",
    "    if outcome == 'ouddx':\n",
    "        print(f\"Mean AUC: {result['mean_auc']:.3f}, 95% CI: [{result['auc_ci_lower']:.3f}, {result['auc_ci_upper']:.3f}]\")\n",
    "        print(f\"Mean Accuracy: {result['mean_accuracy']:.3f}, 95% CI: [{result['accuracy_ci_lower']:.3f}, {result['accuracy_ci_upper']:.3f}]\")\n",
    "    elif outcome == 'commtot':\n",
    "        print(f\"Mean MSE: {result['mean_mse']:.3f}, 95% CI: [{result['mse_ci_lower']:.3f}, {result['mse_ci_upper']:.3f}]\")\n",
    "        print(f\"Mean R: {result['mean_r']:.3f}, 95% CI: [{result['r_ci_lower']:.3f}, {result['r_ci_upper']:.3f}]\")\n",
    "        print(f\"Median R: {np.median(result['r_scores']):.3f}\")    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a81c5",
   "metadata": {},
   "source": [
    "# fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hyperparameter grid for each model\n",
    "\n",
    "models_list = [RandomForestRegressor(), XGBRegressor(), SVR(), Lasso(), Ridge(), ElasticNet(), MLPRegressor()]\n",
    "param_grid_list = [\n",
    "    {\n",
    "        'n_estimators': list(range(50, 500, 50)),\n",
    "        'max_depth': [4, 8, 16],\n",
    "        'min_samples_split': [2, 4, 8],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': list(range(50, 500, 50)),\n",
    "        'max_depth': [4, 8, 16],\n",
    "        'learning_rate': [0.01, 0.1, 0.3]\n",
    "    },\n",
    "    {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': [0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    {\n",
    "        'alpha': [0.1, 0.5, 1, 5, 10],\n",
    "        'max_iter': [100, 200, 300, 400]\n",
    "    },\n",
    "    {\n",
    "        'alpha': [0.1, 0.5, 1, 5, 10],\n",
    "        'max_iter': [100, 200, 300, 400]\n",
    "    },\n",
    "    {\n",
    "        'alpha': [0.1, 0.5, 1, 5, 10],\n",
    "        'l1_ratio': [0.1, 0.5, 0.9],\n",
    "        'max_iter': [100, 200, 300, 400]\n",
    "    },\n",
    "    {\n",
    "        'hidden_layer_sizes': [(100,), (200,), (300,)],\n",
    "        'activation': ['tanh', 'relu', 'logistic'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive', 'invscaling'],\n",
    "        'learning_rate_init': [0.0001, 0.001, 0.005, 0.01, 0.05]\n",
    "    }\n",
    "]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(df, split_data = 'split', outcome = 'commtot', predictor_columns=PREDICTOR_COLUMNS, test_size = test_size, SEED = SEED)\n",
    "\n",
    "for i in enumerate(models_list):\n",
    "    model = i[1]\n",
    "    model_name = model.__class__.__name__\n",
    "    print(model_name)\n",
    "    param_grid = param_grid_list[i[0]]\n",
    "    GS = GridSearchCV(model, param_grid, cv=5, scoring=['r2', 'neg_mean_squared_error'], refit = 'r2', n_jobs=-1, verbose =0)\n",
    "    GS.fit(X_train, y_train)\n",
    "\n",
    "    cv_df = pd.DataFrame(GS.cv_results_)\n",
    "    cv_df.to_csv(model_dir + f'{model_name}_grid_search_results_group-{which_group}.csv', index=False)\n",
    "    joblib.dump(GS, model_dir + f'{model_name}_GS_best_model.pkl')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running model based on the model with the highest r2 score\n",
    "model_results = {}\n",
    "for model in models_list:\n",
    "    model_name = model.__class__.__name__\n",
    "    print(model_name)\n",
    "    cv_df = pd.read_csv(model_dir + f'{model_name}_grid_search_results_group-{which_group}.csv')\n",
    "    best_model_index = cv_df['rank_test_r2'].idxmin()\n",
    "    best_model_param = eval(cv_df.loc[best_model_index, 'params'])\n",
    "    best_model = model.set_params(**best_model_param)\n",
    "    joblib.dump(best_model, model_dir + f'{model_name}_rank_best_model_group-{which_group}.pkl')\n",
    "    model_results[model_name] = mc_cv_ml_pipeline(best_model, df, split='split', outcome='commtot', test_size=0.3, n_split=100, predictor_columns=PREDICTOR_COLUMNS)\n",
    "\n",
    "joblib.dump(model_results, model_dir + f'rank_model_mc_results_group-{which_group}.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba830a",
   "metadata": {},
   "source": [
    "# make bar plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76569acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 rank test\n",
    "# Extract the r_scores for each model\n",
    "which_group = 'high'\n",
    "model_results = joblib.load(model_dir + f'rank_model_mc_results_group-{which_group}.pkl')\n",
    "r_scores = [result['r_scores'] for result in model_results.values()]\n",
    "\n",
    "# model labels:\n",
    "model_labels = [model for model in model_results.keys()]\n",
    "model_labels = [model.replace('Regressor', '') for model in model_labels]\n",
    "# Find the model with the highest mean r_score\n",
    "max_mean_index = np.argmax([results['mean_r'] for results in model_results.values()])\n",
    "\n",
    "# Create a box plot\n",
    "plt.boxplot(r_scores, labels=model_labels, medianprops = dict(color = \"orange\", linewidth = 3))\n",
    "\n",
    "# Add scatter points\n",
    "for i, r_score in enumerate(r_scores):\n",
    "    if i == max_mean_index:\n",
    "        color = 'orange'  # Set the color of the model with the highest mean\n",
    "        print(np.nanmean(r_score))\n",
    "    else:\n",
    "        color = 'gray'  # Set the color of other models to gray\n",
    "    x = np.random.normal(i + 1, 0.04, size=len(r_score))\n",
    "    plt.scatter(x, r_score, alpha=0.5, color=color)\n",
    "\n",
    "# Customize font sizes\n",
    "axis_fontsize = 14\n",
    "xtick_fontsize = 12\n",
    "ytick_fontsize = 12\n",
    "title_fontsize = 16\n",
    "\n",
    "plt.xlabel('Models', fontsize=axis_fontsize)\n",
    "plt.ylabel(\"Pearson's correlation (r)\", fontsize=axis_fontsize)\n",
    "# plt.title('Distribution of correlation values by model', fontsize=title_fontsize)\n",
    "plt.title('Model Performance', fontsize=title_fontsize)\n",
    "\n",
    "plt.xticks(fontsize=xtick_fontsize, rotation=30)\n",
    "plt.yticks(fontsize=ytick_fontsize)\n",
    "\n",
    "\n",
    "# save figure\n",
    "plt.savefig(figures_dir + f'rank_models_box_plot_mean_r_score_group-{which_group}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450dd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 rank test: median r score\n",
    "# Extract the r_scores for each model\n",
    "model_results = joblib.load(model_dir + f'rank_model_mc_results_group-{which_group}.pkl')\n",
    "r_scores = [result['r_scores'] for result in model_results.values()]\n",
    "\n",
    "# model labels:\n",
    "model_labels = [model for model in model_results.keys()]\n",
    "model_labels = [model.replace('Regressor', '') for model in model_labels]\n",
    "# Find the model with the highest mean r_score\n",
    "max_mean_index = np.argmax([results['median_r'] for results in model_results.values()])\n",
    "\n",
    "# Create a box plot\n",
    "plt.boxplot(r_scores, labels=model_labels, medianprops = dict(color = \"orange\", linewidth = 3))\n",
    "\n",
    "# Add scatter points\n",
    "for i, r_score in enumerate(r_scores):\n",
    "    if i == max_mean_index:\n",
    "        color = 'orange'  # Set the color of the model with the highest mean\n",
    "        print(np.median(r_score))\n",
    "    else:\n",
    "        color = 'gray'  # Set the color of other models to gray\n",
    "    x = np.random.normal(i + 1, 0.04, size=len(r_score))\n",
    "    plt.scatter(x, r_score, alpha=0.5, color=color)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('r_score')\n",
    "plt.title('Box Plot of r_score for Each Model')\n",
    "#plt.xticks(rotation=45)\n",
    "\n",
    "# save figure\n",
    "plt.savefig(figures_dir + f'rank_models_box_plot_median_r_score_group-{which_group}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot r2\n",
    "\n",
    "model_results = joblib.load(model_dir + f'rank_model_mc_results_group-{which_group}.pkl')\n",
    "r_scores = [result['r_scores'] for result in model_results.values()]\n",
    "r2_scores = [result['r2_scores'] for result in model_results.values()]\n",
    "# model labels:\n",
    "model_labels = [model for model in model_results.keys()]\n",
    "model_labels = [model.replace('Regressor', '') for model in model_labels]\n",
    "# Find the model with the highest mean r_score\n",
    "max_mean_index = np.argmax([results['median_r2'] for results in model_results.values()])\n",
    "\n",
    "# Create a box plot\n",
    "plt.boxplot(r2_scores, labels=model_labels, medianprops = dict(color = \"orange\", linewidth = 3))\n",
    "\n",
    "# Add scatter points\n",
    "for i, r2_score in enumerate(r2_scores):\n",
    "    if i == max_mean_index:\n",
    "        color = 'orange'  # Set the color of the model with the highest mean\n",
    "        print(np.nanmedian(r2_score))\n",
    "        \n",
    "    else:\n",
    "        color = 'gray'  # Set the color of other models to gray\n",
    "    x = np.random.normal(i + 1, 0.04, size=len(r2_score))\n",
    "    plt.scatter(x, r2_score, alpha=0.5, color=color)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('r2')\n",
    "plt.title('Box Plot of median r2 for Each Model')\n",
    "#plt.xticks(rotation=45)\n",
    "\n",
    "# save figure\n",
    "plt.savefig(figures_dir + f'rank_models_box_plot_median_r2_group-{which_group}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859f87a",
   "metadata": {},
   "source": [
    "## confusion matrix if model is trained on all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeAucPlot(auc_score, fpr,tpr, title):\n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='orange', lw=2, label=f'AUC = {auc_score:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "which_group = 'all'\n",
    "if which_group == 'all':\n",
    "    df = pd.read_csv(data_dir + f'M_K23_ML_group-{which_group}_reduced_imputed.csv')\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "    # find the median model\n",
    "    final_best_model_name = 'ElasticNet'\n",
    "    models_list = [RandomForestRegressor(), XGBRegressor(), SVR(), Lasso(), Ridge(), ElasticNet(), MLPRegressor()]\n",
    "    model = models_list[[model.__class__.__name__ for model in models_list].index(final_best_model_name)]\n",
    "    cv_df = pd.read_csv(model_dir + f'{final_best_model_name}_grid_search_results_group-{which_group}.csv')\n",
    "\n",
    "    best_model_index = cv_df['rank_test_r2'].idxmin()\n",
    "    best_model_param = eval(cv_df.loc[best_model_index, 'params'])\n",
    "    final_best_model = model.set_params(**best_model_param)\n",
    "\n",
    "    print(final_best_model)\n",
    "\n",
    "    # model results:\n",
    "    model_results = joblib.load(model_dir + f'rank_model_mc_results_group-{which_group}.pkl')\n",
    "    r2_scores = model_results[final_best_model_name]['r2_scores']\n",
    "    seed = np.argsort(r2_scores)[len(r2_score)//2-2]\n",
    "    print(f'seed: {seed}')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_data(df, split_data = 'split', outcome = 'commtot', predictor_columns=PREDICTOR_COLUMNS, test_size = test_size, SEED = seed)\n",
    "\n",
    "    \n",
    "    \n",
    "    final_best_model.fit(X_train, y_train)\n",
    "    y_pred = final_best_model.predict(X_test)\n",
    "\n",
    "    print(f'MSE: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "    print(f'R2: {np.corrcoef(y_test, y_pred)[0,1]**2:.3f}')\n",
    "\n",
    "    # binarize y_pred and y_test\n",
    "    y_pred_bin = np.where(y_pred > 9, 1, 0)\n",
    "    y_test_bin = np.where(y_test > 9, 1, 0)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(y_pred, y_test, color='orange')\n",
    "    # add fit line\n",
    "    plt.plot(np.unique(y_pred), np.poly1d(np.polyfit(y_pred, y_test, 1))(np.unique(y_pred)), color='green')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Predicted vs Actual')\n",
    "    plt.savefig(figures_dir + f'{final_best_model_name}_AUC_group-{which_group}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_bin, y_pred)\n",
    "\n",
    "    auc_score = roc_auc_score(y_test_bin, y_pred)\n",
    "    print(f'AUC: {auc_score:.3f}')\n",
    "    MakeAucPlot(auc_score, fpr, tpr, f'ROC Curve: binarize COMM >= 9')\n",
    "\n",
    "    cm = confusion_matrix(y_test_bin, y_pred_bin)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='BuPu', fmt='d', xticklabels=['Low COMM', 'High COMM'], yticklabels=['Low COMM', 'High COMM'],annot_kws={\"size\": 16})\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(figures_dir + f'{final_best_model_name}_ConfusionMatrix_group-{which_group}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # calculate accuracy:\n",
    "    accuracy = np.mean(y_pred_bin == y_test_bin)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    print('chance level: ', 1 - np.mean(y_test_bin))\n",
    "\n",
    "    # calculate sensitivity and specificity\n",
    "    TP = cm[1,1]\n",
    "    TN = cm[0,0]\n",
    "    FP = cm[0,1]\n",
    "    FN = cm[1,0]\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    print(f'Sensitivity: {sensitivity:.4f}')\n",
    "    print(f'Specificity: {specificity:.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ace788",
   "metadata": {},
   "source": [
    "# make ablation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best model from both the rank test\n",
    "\n",
    "final_best_model_name = 'RandomForestRegressor'\n",
    "models_list = [RandomForestRegressor(), XGBRegressor(), SVR(), Lasso(), Ridge(), ElasticNet(), MLPRegressor()]\n",
    "model = models_list[[model.__class__.__name__ for model in models_list].index(final_best_model_name)]\n",
    "\n",
    "cv_df = pd.read_csv(model_dir + f'{final_best_model_name}_grid_search_results_group-{which_group}.csv')\n",
    "best_model_index = cv_df['rank_test_r2'].idxmin()\n",
    "best_model_param = eval(cv_df.loc[best_model_index, 'params'])\n",
    "\n",
    "final_best_model = model.set_params(**best_model_param)\n",
    "\n",
    "final_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acae44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(which_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation = False\n",
    "if ablation:\n",
    "    ablation_results = {}\n",
    "    df = pd.read_csv(data_dir + f'M_K23_ML_group-{which_group}_reduced_imputed.csv')\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    for c in PREDICTOR_COLUMNS:\n",
    "        print(c)\n",
    "        ablated_df = df.copy()\n",
    "        ablated_df[c] = 0\n",
    "        ablation_results[c] = mc_cv_ml_pipeline(final_best_model, ablated_df, split='split', n_split=100, outcome='commtot')\n",
    "        \n",
    "    # save ablation results\n",
    "    joblib.dump(ablation_results, model_dir + f'{final_best_model_name}_ablation_results_group-{which_group}.pkl')    \n",
    "else:\n",
    "    ablation_results = joblib.load(model_dir + f'{final_best_model_name}_ablation_results_group-{which_group}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "r_scores = [result['r_scores'] for result in ablation_results.values()]\n",
    "# Sort the r_scores from low to high\n",
    "r_scores.sort(key=np.mean)\n",
    "# Find the model with the highest mean r_score\n",
    "min_mean_index = np.argmin([np.mean(scores) for scores in r_scores])\n",
    "sorted_ablation_index = np.argsort([np.mean(result['r_scores']) for result in ablation_results.values()])\n",
    "labels = [list(ablation_results.keys())[i] for i in sorted_ablation_index]\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size\n",
    "plt.boxplot(r_scores, labels=labels)\n",
    "\n",
    "# Add scatter points\n",
    "for i, r_score in enumerate(r_scores):\n",
    "    if i == min_mean_index:\n",
    "        color = 'orange'  # Set the color of the model with the highest mean\n",
    "    else:\n",
    "        color = 'gray'  # Set the color of other models to gray\n",
    "    x = np.random.normal(i + 1, 0.04, size=len(r_score))\n",
    "    \n",
    "    plt.scatter(x, r_score, alpha=0.5, color=color)\n",
    "\n",
    "\n",
    "plt.xlabel('Predictors')\n",
    "plt.ylabel('r_score')\n",
    "plt.title('Ablation Analysis for Predictor Impacts')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = labels\n",
    "y = np.mean(delta_r_list, axis=1)\n",
    "yerr = delta_r_ci_list\n",
    "\n",
    "# Sort the data in ascending order\n",
    "sorted_indices = np.argsort(y)\n",
    "x_sorted = np.array(x)[sorted_indices]\n",
    "y_sorted = y[sorted_indices]\n",
    "yerr_sorted = np.array(yerr)[sorted_indices]\n",
    "\n",
    "\n",
    "axis_fontsize = 14\n",
    "xtick_fontsize = 12\n",
    "ytick_fontsize = 12\n",
    "title_fontsize = 16\n",
    "\n",
    "\n",
    "# Plotting horizontal bar graph with error bars\n",
    "plt.figure(figsize=(6, 12))\n",
    "plt.barh(x_sorted, y_sorted, xerr=np.abs(yerr_sorted), capsize=5, color='orange', edgecolor='black')\n",
    "plt.ylabel('Predictors', fontsize=axis_fontsize)  # Horizontal bar plot has ylabel for predictors\n",
    "plt.xlabel('delta r after ablation', fontsize=axis_fontsize)\n",
    "plt.title('Feature Ablation Impacts on Prediction Performance', fontsize=title_fontsize)\n",
    "plt.xticks(fontsize=xtick_fontsize)\n",
    "plt.yticks(fontsize=ytick_fontsize)\n",
    "# Show the plot\n",
    "plt.savefig(figures_dir + f'{final_best_model_name}_ablation_analysis_group-{which_group}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "df = pd.read_csv(data_dir + f'M_K23_ML_group-{which_group}_reduced_imputed.csv')\n",
    "X_train, X_test, y_train, y_test = split_data(df, split_data = 'split', outcome = 'commtot', predictor_columns=PREDICTOR_COLUMNS, test_size = test_size, SEED=SEED)\n",
    "model = joblib.load(model_dir + f'{final_best_model_name}_rank_best_model.pkl')\n",
    "model.fit(X_train, y_train)\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, show=False, max_display=11)\n",
    "\n",
    "# save the shap figure\n",
    "plt.savefig(figures_dir + f'{final_best_model_name}_shap_summary_plot_group-{which_group}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a5a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b60ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e276f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ebce92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
